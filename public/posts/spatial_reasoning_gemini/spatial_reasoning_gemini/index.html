<!DOCTYPE html>
<html><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8"/>
    <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
    <link href="/images/fav/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
    <link href="/images/fav/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
    <link href="/images/fav/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
    <link href="/images/fav/site.webmanifest" rel="manifest"/>
    <title>Gemini 2.5 Spatial Reasoning | My Modern Blog</title>
    <link href="/css/styles.css" rel="stylesheet"/>
    <link href="/css/custom.css" rel="stylesheet"/>
    
</head><body><header>
    <div class="logo h-card">
        <a class="u-url" href="http://localhost:1313/">
            <img alt="Site Logo" class="site-logo u-photo" src="/images/logo.png"/>
        </a>
    </div>
    <nav class="menu">
        <ul>
        </ul>
    </nav>
</header>

    

    <section class="hero"
    style='background-image: url("/images/3.jpg"); background-size: cover; background-position: center; background-repeat: no-repeat; width: 100%;'
    id="heroSection" onload="adjustTextColor(this)">
        <div>
            <h1 class="p-name" id="textOverlay">Gemini 2.5 Spatial Reasoning</h1>
        </div>
    </section>
    <main class="post h-entry container row">
        <div class="post">
            <div class="post-data row">
                <time class="dt-published" datetime="2025-04-09 09:23:00 &#43;0200 CEST">
                    <a class="u-url" href="http://localhost:1313/posts/spatial_reasoning_gemini/spatial_reasoning_gemini/">Apr 9, 2025</a>
                </time>
                <span rel="author" class="p-author h-card">
                    <img class="u-photo hidden" src="" />
                    <span class="p-name hidden" rel="me"></span>
                </span>
            </div>
            <div class="e-content">
                
<figure class="figure">
  
    <div style="color:red;">Image not found: fox_gemini.png</div>
  
</figure>

<h1 id="gemini-spatial-reasoning-is-amazing">Gemini Spatial Reasoning Is Amazing</h1>
<p><strong>&hellip;but not good enough to be helpful yet.</strong></p>
<p>Various large language models with vision support enable users to ask questions about images, usually referred to as [visual reasoning]. This allows queries like: <em>How many pumpkins do you see in this picture?</em> or <em>What kind of flower is in this picture?</em> This is already very useful, as it enables zero-shot classification without needing to train a model.</p>
<p>However, when you need to determine <strong>where</strong> that pumpkin or flower is in the picture, it&rsquo;s not possible—yet. That’s where <strong>spatial reasoning</strong> comes in, allowing models to return bounding boxes or coordinates for objects through prompting.</p>
<p>Gemini 2.5 is not the first model to enable this. For years, most vision models have been built on top of transformer architectures and allow zero-shot prompting to some degree, like MobileCLIP, SigLIP v2, or GroundingDINO—the latter even enabling bounding boxes, not just classification.</p>
<p>Additionally, visual grounding was possible with other vision-supporting LLMs like GPT-4o, but building such pipelines was complex and, in my opinion, not worth the effort compared to fine-tuning a model like <a href="https://arxiv.org/abs/2407.17140">RT-DETR</a>.</p>
<p>The transformer architectures that make this possible are typically the basis of the vision encoder in LLMs. For example:</p>
<ul>
<li>LLaMA 4 uses the MetaCLIP architecture</li>
<li>Gemma 3 uses SigLIP</li>
<li>Qwen 2.5 uses ViT</li>
</ul>
<p>Spatial reasoning is not something Gemini 2.5 introduced to LLMs—it was already available in Gemini 2.0 Flash and in open-source models like Qwen2.5-VL. However, given that Gemini 2.5 is dominating benchmarks like <em>HumanEval</em> and <em>MMMU</em>, I’d argue it&rsquo;s the best model for testing visual reasoning in LLMs today.</p>
<p>If you just want to play around with spatial reasoning, try the Google demo <a href="https://aistudio.google.com/starter-apps/spatial">here</a>—but note that it uses Gemini 2.0 Flash, not 2.5 Pro. If you want to use Gemini 2.5 Pro and test experimental features like semantic segmentation, you’ll need to use Google’s API directly. Google provides an excellent Jupyter Notebook <a href="https://github.com/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb">here</a>, which I’ll reference for implementation tips.</p>
<hr>
<h2 id="how-to-use-geminis-spatial-reasoning-most-effectively">How to Use Gemini’s Spatial Reasoning Most Effectively</h2>
<p>Example Prompt:</p>
<blockquote>
<p>Detect big stones, with no more than 20 items. Output a JSON list where each entry contains the 2D bounding box in <code>&quot;box_2d&quot;</code> and a text label in <code>&quot;label&quot;</code>.</p></blockquote>
<p>TODO</p>
<h3 id="1-use-the-experimental-version-of-gemini-20-flash-or-25-pro">1. Use the experimental version of Gemini 2.0 Flash or 2.5 Pro</h3>
<p>One of my first observations: the default model 2.0 Flash model performs much worse. While the predictions of something being present might be okay, it often fails at the visual grounding. In other words the <em>bounding boxes</em> are wrong in the wrong place.</p>
<p>The experimental Gemini 2.0 Flash (<code>gemini-2.0-flash-exp</code>) performs significantly better than the default model (<code>gemini-2.0-flash</code>, aka <code>gemini-2.0-flash-001</code>) and is also the version used in the web demo.</p>
<p>For Gemini 2.5 Pro, I only had access to the experimental version: <code>gemini-2.5-pro-exp-03-25</code> and I was disappointed, but more on that later.</p>
<h3 id="2-downscale-images-to-640px">2. Downscale images to 640px</h3>
<p>Higher image resolutions lead to worse performance—likely due to the vision encoder limits. Assuming Gemini also uses SigLIP (as used in Gemma 3), that model has a max resolution of 512px.</p>
<p>The Aistudio app downscales images to 640px automatically (<a href="https://github.com/google-gemini/starter-applets/blob/main/spatial/src/Prompt.tsx#L71">see here</a>).<br>
I’m not sure what the optimal resolution is, but I’d recommend sticking to 512px or 640px.</p>
<p>Sending higher-res images via the API results in <strong>worse grounding</strong> and <strong>more unreproducable</strong> outputs. 640px works well and yields consistent results.<br>
If you need higher detail, consider splitting the image into patches and doing prediction on each—this technique is known as <strong>SAHI (Slicing Aided Hyper Inference).</strong></p>
<p>The combo of <code>gemini-2.0-flash-exp</code> and 640px resolution performs the reliable in my test - at least on photos and data similar to the COCO dataset. Still, I recommend adapting some ideas from Google&rsquo;s notebook.</p>
<h3 id="3-loosen-the-generation-guardrails">3. Loosen the generation guardrails</h3>
<p>The default safety filters are often too sensitive to this kind of output. So you can make them less stict like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>safety_settings <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    genai<span style="color:#f92672">.</span>types<span style="color:#f92672">.</span>SafetySetting(
</span></span><span style="display:flex;"><span>        category<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HARM_CATEGORY_DANGEROUS_CONTENT&#34;</span>,
</span></span><span style="display:flex;"><span>        threshold<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;BLOCK_ONLY_HIGH&#34;</span>,
</span></span><span style="display:flex;"><span>    ),
</span></span><span style="display:flex;"><span>]
</span></span></code></pre></div><h3 id="4-use-a-task-specific-system-prompt">4. Use a task-specific system prompt</h3>
<p>There are different system prompts for:</p>
<ul>
<li>Pointing</li>
<li>2D boxes</li>
<li>3D boxes</li>
<li>Segmentation</li>
</ul>
<p>Google suggests 2D boxes give the most reliable results. I usually go with that. The following is the system prompt I used for my tests.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>bounding_box_system_instructions <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Return bounding boxes as a JSON array with labels. Never return masks or code fencing. Limit to 25 objects.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">If an object is present multiple times, name them according to their unique characteristic (colors, size, position, unique characteristics, etc..).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">If there are no relevant objects present answer with: &#34;Nothing found&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span></code></pre></div><p>The system prompts for the other task you can also find in Googles Notebook</p>
<h3 id="5-use-a-low-temperature--05">5. Use a low temperature (&lt;= 0.5)</h3>
<p>Lower temperatures make the output less creative and more consistent. If you combine all of this you should end up with a API request like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>response <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>generate_content(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gemini-2.0-flash-exp&#34;</span>,
</span></span><span style="display:flex;"><span>    contents<span style="color:#f92672">=</span>[prompt, image],
</span></span><span style="display:flex;"><span>    config <span style="color:#f92672">=</span> genai<span style="color:#f92672">.</span>types<span style="color:#f92672">.</span>GenerateContentConfig(
</span></span><span style="display:flex;"><span>        system_instruction<span style="color:#f92672">=</span>bounding_box_system_instructions,
</span></span><span style="display:flex;"><span>        temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>,
</span></span><span style="display:flex;"><span>        safety_settings<span style="color:#f92672">=</span>safety_settings,
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><hr>
<h2 id="some-examplesand-why-i-got-disappointed">Some Examples—and Why I Got Disappointed</h2>
<p>Let’s start with the example from <a href="https://www.youtube.com/watch?v=-XmoDzDMqj4">Google’s introduction video</a>: the fox’s shadow.</p>
<div class="figures-row">
  
  <figure class="figure">
    
      <div style="color:red;">Image not found: fox_gemini.png</div>
    
  </figure>

  
  <figure class="figure">
    
      <div style="color:red;">Image not found: fox_image.png</div>
    
  </figure>

  
  <figure class="figure">
    
      <div style="color:red;">Image not found: fox_groundingdino.png</div>
    
  </figure>

</div>
<p>Prompt used:</p>
<ul>
<li>
<p>Detect fox shadow, with no more than 20 items. Output a json list where each entry contains the 2D bounding box in &ldquo;box_2d&rdquo; and a text label in &ldquo;label&rdquo;.</p>
</li>
<li>
<p>Detect the fox&rsquo;s shadow, with no more than 20 items. Output a JSON list where each entry contains the 2D bounding box in &ldquo;box_2d&rdquo; and a text label in &ldquo;label&rdquo;.</p>
</li>
</ul>
<p>It matters a bit how you prompt it because if you ask &ldquo;Detect fox shadow,&hellip;&rdquo; it will not work. It only works if you ask &ldquo;Detect the fox&rsquo;s shadow, &hellip;&rdquo; in that way the results can be a bit unpredictable.</p>
<p>Here’s the thing—you don’t need Gemini for this. Other zero-shot models perform just as well at identifying it. Above prediction was done by <a href="https://huggingface.co/IDEA-Research/grounding-dino-base">GroundingDino</a> a 172M parameter model.</p>
<p>So I tried something more project-specific: detecting a trench.</p>
<ul>
<li>Gemini didn’t do badly.</li>
<li>But the lightweight zero-shot model on my local machine did better visual grounding.</li>
</ul>
<p>Next, I tried something that <em>should</em> have been easy: detecting the stones under an orange cable.<br>
Turns out—it wasn’t.</p>
<ul>
<li>Gemini either detected nothing or hallucinated irrelevant objects.</li>
<li>Results were inconsistent — every prediction yielded different results</li>
<li>Switching form 2D boxes to pointing didn&rsquo;t help</li>
<li>GroundingDINO and OWL-ViT also failed, but at least they were consistently failing, which is arguably better than hallucinating.</li>
</ul>
<p>I thought maybe the stones were too small, but patch-based (SAHI) inference didn’t help either.<br>
In fact, Gemini started hallucinating—detecting stones where there were none, which is a bigger issue than missing a few.</p>
<p>And oddly, Gemini 2.0 Flash outperformed 2.5 Pro in this case.<br>
This doesn’t necessarily mean 2.0 is better overall—but it suggests the spatial understanding component is trained differently between versions.</p>
<p>At this point, I realized that for my needs, Geminis spatial reasoning isn’t good enough. The only real solution would be to fine-tune an object detection model for the specific class “stone”.</p>
<hr>
<h2 id="some-unsatisfying-conclusions">Some Unsatisfying Conclusions</h2>
<p>Gemini’s spatial reasoning is impressive. You write a prompt—and it just works, for simple use-cases. No model training, just API calls.</p>
<p>But Gemini isn’t the only model supporting zero-shot, open-world object detection.<br>
<strong>OWL-ViT, OWL2, and GroundingDINO</strong> are good alternatives: smaller, cheaper, and runnable on-device.<br>
And I’d argue they’re just as good as Gemini for zero-shot object detection or other spatial reasoning tasks. Only on complex reasoning Gemini has the edge, eg. if you query is very complex.</p>
<p>Eventually, foundation models will surpass smaller, fine-tuned models.<br>
But that time isn’t here yet.</p>
<p>Frankly, I’m disappointed by Gemini 2.5—especially given how stable 2.0 feels.</p>
<p>So for now, in 2025, I still need to train CV models on specific datasets, which is tedious—but necessary.</p>

                
                <span class="tag-container">
                    
                    <a class="p-category tag" href="/tags/gemini-2.5/">
                        Gemini-2.5
                    </a>&nbsp;
                    
                    <a class="p-category tag" href="/tags/gemini/">
                        Gemini
                    </a>&nbsp;
                    
                </span>
                
            </div>
        </div>
        
    </main>
    <script>
        function adjustTextColor(sectionId) {
            const section = document.getElementById(sectionId);
            const style = window.getComputedStyle(section);
            const imageUrl = style
                .backgroundImage
                .slice(4, -1)
                .replace(/["']/g, "");
            const image = new Image();
            image.src = imageUrl;
            image.onload = function () {
                const canvas = document.createElement('canvas');
                const context = canvas.getContext('2d');
                canvas.width = image.width;
                canvas.height = image.height;
                context.drawImage(image, 0, 0, image.width, image.height);
                const imageData = context.getImageData(0, 0, canvas.width, canvas.height);
                const data = imageData.data;
                let r = 0,
                    g = 0,
                    b = 0;
                for (let i = 0; i < data.length; i += 4) {
                    r += data[i];
                    g += data[i + 1];
                    b += data[i + 2];
                }
                r /= (data.length / 4);
                g /= (data.length / 4);
                b /= (data.length / 4);
                const brightness = Math.sqrt(0.299 * r * r + 0.587 * g * g + 0.114 * b * b);
                const textColor = brightness > 128
                    ? 'black'
                    : 'white';
                const shadowColor = brightness > 128
                    ? 'rgba(255, 255, 255, 0.7)'
                    : 'rgba(0, 0, 0, 0.7)';
                const textOverlay = document.getElementById('textOverlay');
                textOverlay.style.color = textColor;
                textOverlay.style.textShadow = `0 0 5px ${shadowColor}, 0 0 10px ${shadowColor}, 0 0 20px ${shadowColor}, 0 0 40px ${shadowColor}, 0 0 80px ${shadowColor}, 0 0 90px ${shadowColor}, 0 0 100px ${shadowColor}, 0 0 150px ${shadowColor}`;
            };
        }
        
        adjustTextColor('heroSection');
    </script>
    <script>
        window.addEventListener('DOMContentLoaded', () => {
            const observerForTableOfContentActiveState = new IntersectionObserver(entries => {
                entries.forEach(entry => {
                    const id = entry
                        .target
                        .getAttribute('id');

                    if (entry.intersectionRatio > 0) {
                        clearActiveStatesInTableOfContents();
                        document
                            .querySelector(`nav li a[href="#${id}"]`)
                            .parentElement
                            .classList
                            .add('active');
                    }
                });
            });
            document
                .querySelectorAll('h1[id],h2[id],h3[id],h4[id]')
                .forEach((section) => {
                    observerForTableOfContentActiveState.observe(section);
                });

        });

        function clearActiveStatesInTableOfContents() {
            document
                .querySelectorAll('nav li')
                .forEach((section) => {
                    section
                        .classList
                        .remove('active');
                });
        }
    </script>
<footer>
    <div class="social-media">
        
            <a href="https://github.com/yourusername"><img alt="GitHub" src=""/></a>
        
            <a href="https://twitter.com/yourusername"><img alt="Twitter" src=""/></a>
        
    </div>
    <div class="logo h-card">
        <a class="u-url" href="http://localhost:1313/">
            <img alt="Site Logo" class="site-logo u-photo" src="/images/logo.png"/>
        </a>
        <p>© 2025
            My Modern Blog</p>
    </div>
    <nav>
        <ul>
        </ul>
    </nav>
    <script>
    window.store = {
    
    
    "http:\/\/localhost:1313\/tags\/gemini\/": {
        
        "title": "Gemini",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/gemini\/"
        },
    
    
    "http:\/\/localhost:1313\/posts\/spatial_reasoning_gemini\/spatial_reasoning_gemini\/": {
        
        "title": "Gemini 2.5 Spatial Reasoning",
            "tags": [ "Gemini-2.5",  "Gemini", ],
    "content": " Image not found: fox_gemini.png Gemini Spatial Reasoning Is Amazing \u0026hellip;but not good enough to be helpful yet.\nVarious large language models with vision support enable users to ask questions about images, usually referred to as [visual reasoning]. This allows queries like: How many pumpkins do you see in this picture? or What kind of flower is in this picture? This is already very useful, as it enables zero-shot classification without needing to train a model.\nHowever, when you need to determine where that pumpkin or flower is in the picture, it\u0026rsquo;s not possible—yet. That’s where spatial reasoning comes in, allowing models to return bounding boxes or coordinates for objects through prompting.\nGemini 2.5 is not the first model to enable this. For years, most vision models have been built on top of transformer architectures and allow zero-shot prompting to some degree, like MobileCLIP, SigLIP v2, or GroundingDINO—the latter even enabling bounding boxes, not just classification.\nAdditionally, visual grounding was possible with other vision-supporting LLMs like GPT-4o, but building such pipelines was complex and, in my opinion, not worth the effort compared to fine-tuning a model like RT-DETR.\nThe transformer architectures that make this possible are typically the basis of the vision encoder in LLMs. For example:\nLLaMA 4 uses the MetaCLIP architecture Gemma 3 uses SigLIP Qwen 2.5 uses ViT Spatial reasoning is not something Gemini 2.5 introduced to LLMs—it was already available in Gemini 2.0 Flash and in open-source models like Qwen2.5-VL. However, given that Gemini 2.5 is dominating benchmarks like HumanEval and MMMU, I’d argue it\u0026rsquo;s the best model for testing visual reasoning in LLMs today.\nIf you just want to play around with spatial reasoning, try the Google demo here—but note that it uses Gemini 2.0 Flash, not 2.5 Pro. If you want to use Gemini 2.5 Pro and test experimental features like semantic segmentation, you’ll need to use Google’s API directly. Google provides an excellent Jupyter Notebook here, which I’ll reference for implementation tips.\nHow to Use Gemini’s Spatial Reasoning Most Effectively Example Prompt:\nDetect big stones, with no more than 20 items. Output a JSON list where each entry contains the 2D bounding box in \u0026quot;box_2d\u0026quot; and a text label in \u0026quot;label\u0026quot;.\nTODO\n1. Use the experimental version of Gemini 2.0 Flash or 2.5 Pro One of my first observations: the default model 2.0 Flash model performs much worse. While the predictions of something being present might be okay, it often fails at the visual grounding. In other words the bounding boxes are wrong in the wrong place.\nThe experimental Gemini 2.0 Flash (gemini-2.0-flash-exp) performs significantly better than the default model (gemini-2.0-flash, aka gemini-2.0-flash-001) and is also the version used in the web demo.\nFor Gemini 2.5 Pro, I only had access to the experimental version: gemini-2.5-pro-exp-03-25 and I was disappointed, but more on that later.\n2. Downscale images to 640px Higher image resolutions lead to worse performance—likely due to the vision encoder limits. Assuming Gemini also uses SigLIP (as used in Gemma 3), that model has a max resolution of 512px.\nThe Aistudio app downscales images to 640px automatically (see here).\nI’m not sure what the optimal resolution is, but I’d recommend sticking to 512px or 640px.\nSending higher-res images via the API results in worse grounding and more unreproducable outputs. 640px works well and yields consistent results.\nIf you need higher detail, consider splitting the image into patches and doing prediction on each—this technique is known as SAHI (Slicing Aided Hyper Inference).\nThe combo of gemini-2.0-flash-exp and 640px resolution performs the reliable in my test - at least on photos and data similar to the COCO dataset. Still, I recommend adapting some ideas from Google\u0026rsquo;s notebook.\n3. Loosen the generation guardrails The default safety filters are often too sensitive to this kind of output. So you can make them less stict like this:\nsafety_settings = [ genai.types.SafetySetting( category=\u0026#34;HARM_CATEGORY_DANGEROUS_CONTENT\u0026#34;, threshold=\u0026#34;BLOCK_ONLY_HIGH\u0026#34;, ), ] 4. Use a task-specific system prompt There are different system prompts for:\nPointing 2D boxes 3D boxes Segmentation Google suggests 2D boxes give the most reliable results. I usually go with that. The following is the system prompt I used for my tests.\nbounding_box_system_instructions = \u0026#34;\u0026#34;\u0026#34; Return bounding boxes as a JSON array with labels. Never return masks or code fencing. Limit to 25 objects. If an object is present multiple times, name them according to their unique characteristic (colors, size, position, unique characteristics, etc..). If there are no relevant objects present answer with: \u0026#34;Nothing found\u0026#34; \u0026#34;\u0026#34;\u0026#34; The system prompts for the other task you can also find in Googles Notebook\n5. Use a low temperature (\u0026lt;= 0.5) Lower temperatures make the output less creative and more consistent. If you combine all of this you should end up with a API request like this:\nresponse = client.models.generate_content( model=\u0026#34;gemini-2.0-flash-exp\u0026#34;, contents=[prompt, image], config = genai.types.GenerateContentConfig( system_instruction=bounding_box_system_instructions, temperature=0.5, safety_settings=safety_settings, ) ) Some Examples—and Why I Got Disappointed Let’s start with the example from Google’s introduction video: the fox’s shadow.\nImage not found: fox_gemini.png Image not found: fox_image.png Image not found: fox_groundingdino.png Prompt used:\nDetect fox shadow, with no more than 20 items. Output a json list where each entry contains the 2D bounding box in \u0026ldquo;box_2d\u0026rdquo; and a text label in \u0026ldquo;label\u0026rdquo;.\nDetect the fox\u0026rsquo;s shadow, with no more than 20 items. Output a JSON list where each entry contains the 2D bounding box in \u0026ldquo;box_2d\u0026rdquo; and a text label in \u0026ldquo;label\u0026rdquo;.\nIt matters a bit how you prompt it because if you ask \u0026ldquo;Detect fox shadow,\u0026hellip;\u0026rdquo; it will not work. It only works if you ask \u0026ldquo;Detect the fox\u0026rsquo;s shadow, \u0026hellip;\u0026rdquo; in that way the results can be a bit unpredictable.\nHere’s the thing—you don’t need Gemini for this. Other zero-shot models perform just as well at identifying it. Above prediction was done by GroundingDino a 172M parameter model.\nSo I tried something more project-specific: detecting a trench.\nGemini didn’t do badly. But the lightweight zero-shot model on my local machine did better visual grounding. Next, I tried something that should have been easy: detecting the stones under an orange cable.\nTurns out—it wasn’t.\nGemini either detected nothing or hallucinated irrelevant objects. Results were inconsistent — every prediction yielded different results Switching form 2D boxes to pointing didn\u0026rsquo;t help GroundingDINO and OWL-ViT also failed, but at least they were consistently failing, which is arguably better than hallucinating. I thought maybe the stones were too small, but patch-based (SAHI) inference didn’t help either.\nIn fact, Gemini started hallucinating—detecting stones where there were none, which is a bigger issue than missing a few.\nAnd oddly, Gemini 2.0 Flash outperformed 2.5 Pro in this case.\nThis doesn’t necessarily mean 2.0 is better overall—but it suggests the spatial understanding component is trained differently between versions.\nAt this point, I realized that for my needs, Geminis spatial reasoning isn’t good enough. The only real solution would be to fine-tune an object detection model for the specific class “stone”.\nSome Unsatisfying Conclusions Gemini’s spatial reasoning is impressive. You write a prompt—and it just works, for simple use-cases. No model training, just API calls.\nBut Gemini isn’t the only model supporting zero-shot, open-world object detection.\nOWL-ViT, OWL2, and GroundingDINO are good alternatives: smaller, cheaper, and runnable on-device.\nAnd I’d argue they’re just as good as Gemini for zero-shot object detection or other spatial reasoning tasks. Only on complex reasoning Gemini has the edge, eg. if you query is very complex.\nEventually, foundation models will surpass smaller, fine-tuned models.\nBut that time isn’t here yet.\nFrankly, I’m disappointed by Gemini 2.5—especially given how stable 2.0 feels.\nSo for now, in 2025, I still need to train CV models on specific datasets, which is tedious—but necessary.\n", 
    "url": "http:\/\/localhost:1313\/posts\/spatial_reasoning_gemini\/spatial_reasoning_gemini\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/gemini-2.5\/": {
        
        "title": "Gemini-2.5",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/gemini-2.5\/"
        },
    
    
    "http:\/\/localhost:1313\/": {
        
        "title": "My Modern Blog",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/"
        },
    
    
    "http:\/\/localhost:1313\/posts\/": {
        
        "title": "Posts",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/posts\/"
        },
    
    
    "http:\/\/localhost:1313\/tags\/": {
        
        "title": "Tags",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/tags\/"
        },
    
    
    "http:\/\/localhost:1313\/categories\/": {
        
        "title": "Categories",
            "tags": [],
    "content": "", 
    "url": "http:\/\/localhost:1313\/categories\/"
        },
    
    }
</script>

<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>

</footer></body>
</html>
